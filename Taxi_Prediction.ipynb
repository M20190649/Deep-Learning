{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Taxi Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PtIu8GKrs3Ny",
        "Pba-1Evtt8p3",
        "8US6Gdx9xcX9",
        "px4ojx3xci71"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cedricclg/Deep-Learning/blob/master/Taxi_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj-r-LXPGEQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import json\n",
        "\n",
        "from sklearn.cluster import MeanShift\n",
        "\n",
        "import time\n",
        "from datetime import datetime as dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXY2RuUY31I4",
        "colab_type": "text"
      },
      "source": [
        "Runtime > Change runtime type > Hardware acceleration: GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jjlei6Bptup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fllbaYVIpue7",
        "colab_type": "code",
        "outputId": "c53c88b5-cdee-4ad2-c723-588b7bfdb3a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUGHSEYf4rMJ",
        "colab_type": "text"
      },
      "source": [
        "Path to the data in Google Drive:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Kl8bxKGJZy",
        "colab_type": "code",
        "outputId": "eb80c938-2290-487e-fa01-9dd212cfa9da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/My Drive/Deep-Learning/Deep-Learning/Data/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTffige8GV-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(file):\n",
        "  df = pd.read_csv(data_path + file)\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtIu8GKrs3Ny",
        "colab_type": "text"
      },
      "source": [
        "# **Validation and testing sets**\n",
        "\n",
        "Shuffle the data, extract and save the new sets (CAN BE RUN ONLY ONCE, THE NEW DATA FILES ARE LOADED IN THE NEXT SECTIONS) \\\\\n",
        "Validation: 20070 trajectories \\\\\n",
        "Testing: 20000 trajectories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ym1zUPHwwcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = get_data(\"train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWNnOHG7w6EO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = train.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEGUTQkSw9MD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid = train.iloc[:20070].reset_index(drop=True)\n",
        "test = train.iloc[20070:40070].reset_index(drop=True)\n",
        "train = train.iloc[40070:].reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGCOni-Qw_rR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.to_csv(data_path + \"train_new.csv\")\n",
        "valid.to_csv(data_path + \"valid_new.csv\")\n",
        "test.to_csv(data_path + \"test_new.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pba-1Evtt8p3",
        "colab_type": "text"
      },
      "source": [
        "# **Clustering**\n",
        "\n",
        "Find and save the clusters for all the destinations of the new training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4riwH0ry1g5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = get_data(\"train_new.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I648Z9wZqNKy",
        "colab_type": "text"
      },
      "source": [
        "Convert a list of strings into a list of coordinates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72xceR9j8EmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def string_to_list(list_of_string):\n",
        "  list_total = []\n",
        "  for string in list_of_string:\n",
        "    list_total.append(json.loads(string))\n",
        "  return list_total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzmxy_xMqViX",
        "colab_type": "text"
      },
      "source": [
        "Extract the last coordinates of a string of successive coordinates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAl1pxf5qIg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def destination(string):\n",
        "  l = len(string)\n",
        "  if l!=2:\n",
        "    for i in range(l):\n",
        "      if string[l-i-1]=='[':\n",
        "        new_string = string[l-i-1:l-1]\n",
        "        break\n",
        "  else:\n",
        "    new_string = string\n",
        "  dest = json.loads(new_string)\n",
        "  dest = np.array(dest)\n",
        "  return dest"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7GY5U2Sxxd6",
        "colab_type": "text"
      },
      "source": [
        "All the destinations of the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkvL6rq6qbxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n = len(train['POLYLINE'])\n",
        "\n",
        "destinations = np.zeros((n,2))\n",
        "for i in range(n):\n",
        "  if len(destination(train['POLYLINE'][i])) !=0: # Check if the trajectory (and the destination) actually exists\n",
        "    destinations[i,:] = destination(train['POLYLINE'][i])\n",
        "  else:\n",
        "    destinations[i,:] = np.array([-8.611317,41.146504]) # If not, we manually choose the destination to be the main avenue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEwPhPhkbNLE",
        "colab_type": "code",
        "outputId": "e3ca529b-b115-4706-8251-e78f101c5f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(destinations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-8.594073 41.161752]\n",
            " [-8.605467 41.126229]\n",
            " [-8.646741 41.148468]\n",
            " ...\n",
            " [-8.636535 41.206464]\n",
            " [-8.636994 41.172237]\n",
            " [-8.559027 41.231844]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FqJSWFleUJy",
        "colab_type": "text"
      },
      "source": [
        "Number of samples (destinations) used for the clustering:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r1d1U8geLxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#n_samples_clusters = 300000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELc7vbOHy-mz",
        "colab_type": "text"
      },
      "source": [
        "Run the clustering algorithm and save the results (CAN BE RUN ONLY ONCE, THE CLUSTERS ARE THEN LOADED IN THE NEXT SECTIONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p18wtIb0bcbm",
        "colab_type": "text"
      },
      "source": [
        "Mean-Shift algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNdSTOs9SJR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mean_dest = np.mean(destinations,axis=0)\n",
        "#std_dest = np.sqrt(np.var(destinations))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZe4_MBf4Uu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#centers = clustering.cluster_centers_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcebVBMThsMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#C = centers.shape[0]\n",
        "#print(C)\n",
        "#print(centers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSNX3MfWg_mL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#np.save(data_path+\"centers\", centers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBzZAY4ubOFf",
        "colab_type": "text"
      },
      "source": [
        "A (much) faster method for clustering, over all the training set: K-means algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm5kQ8W5NlAW",
        "colab_type": "text"
      },
      "source": [
        "Install packages (run twice if it does not work):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5KCzSXSNj_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install sphinx > install.log\n",
        "!pip install pykeops[full] > install.log\n",
        "\n",
        "from pykeops.torch import LazyTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUHsxR58bjz4",
        "colab_type": "text"
      },
      "source": [
        "Desired number of clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUHrS3Tbbuq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C = 3000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JYkRJr1Zm4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KMeans(x, K=10, Niter=10, verbose=True):\n",
        "    N, D = x.shape  # Number of samples, dimension of the ambient space\n",
        "\n",
        "    # K-means loop:\n",
        "    # - x  is the point cloud,\n",
        "    # - cl is the vector of class labels\n",
        "    # - c  is the cloud of cluster centroids\n",
        "    start = time.time()\n",
        "    c = x[:K, :].clone()  # Simplistic random initialization\n",
        "    x_i = LazyTensor(x[:, None, :])  # (Npoints, 1, D)\n",
        "\n",
        "    for i in range(Niter):\n",
        "\n",
        "        c_j = LazyTensor(c[None, :, :])  # (1, Nclusters, D)\n",
        "        D_ij = ((x_i - c_j) ** 2).sum(-1)  # (Npoints, Nclusters) symbolic matrix of squared distances\n",
        "        cl = D_ij.argmin(dim=1).long().view(-1)  # Points -> Nearest cluster\n",
        "\n",
        "        Ncl = torch.bincount(cl).type(torch.float64)  # Class weights\n",
        "        for d in range(D):  # Compute the cluster centroids with torch.bincount:\n",
        "            c[:, d] = torch.bincount(cl, weights=x[:, d]) / Ncl\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    if verbose:\n",
        "        print(\"K-means example with {:,} points in dimension {:,}, K = {:,}:\".format(N, D, K))\n",
        "        print('Timing for {} iterations: {:.5f}s = {} x {:.5f}s\\n'.format(\n",
        "                Niter, end - start, Niter, (end-start) / Niter))\n",
        "\n",
        "    return cl, c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0NHZ0QXZ5gZ",
        "colab_type": "code",
        "outputId": "d249a15e-5167-4e33-9233-f974af45c998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "cl, centers = KMeans(torch.from_numpy(destinations).to(device),K=C)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compiling libKeOpstorchd4132d0a01 in /root/.cache/pykeops-1.3-cpython-36/build-libKeOpstorchd4132d0a01:\n",
            "       formula: ArgMin_Reduction(Sum(Square((Var(0,2,0) - Var(1,2,1)))),0)\n",
            "       aliases: Var(0,2,0); Var(1,2,1); \n",
            "       dtype  : float64\n",
            "... Done.\n",
            "K-means example with 1,670,600 points in dimension 2, K = 3,000:\n",
            "Timing for 10 iterations: 33.75463s = 10 x 3.37546s\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuO9TpcWazRG",
        "colab_type": "code",
        "outputId": "b8f3b519-70b4-4a6a-a021-50b1531718da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(centers)\n",
        "print(centers.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-8.5941, 41.1617],\n",
            "        [-8.6055, 41.1276],\n",
            "        [-8.6460, 41.1442],\n",
            "        ...,\n",
            "        [-8.6542, 41.1806],\n",
            "        [-8.6352, 41.1881],\n",
            "        [-8.6012, 41.1816]], device='cuda:0', dtype=torch.float64)\n",
            "torch.Size([3000, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lsg9Lr0Wg9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "centers = centers.cpu().numpy()\n",
        "centers = centers[~np.isnan(centers).any(axis=1),:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GSWVn9jYVwu",
        "colab_type": "code",
        "outputId": "ae1b945e-7c95-4832-8fbf-46069aa54eb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(centers)\n",
        "C = centers.shape[0]\n",
        "print(C)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-8.59411598 41.16172906]\n",
            " [-8.60547552 41.12759636]\n",
            " [-8.64602132 41.14424992]\n",
            " ...\n",
            " [-8.65419941 41.18062638]\n",
            " [-8.63515432 41.18811249]\n",
            " [-8.60124582 41.18163732]]\n",
            "(2984, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNWXD8swb8gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(data_path+\"centers_K\", centers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8US6Gdx9xcX9",
        "colab_type": "text"
      },
      "source": [
        "# **Define and train the model (MLP)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfphS21oqd7f",
        "colab_type": "text"
      },
      "source": [
        "Distances relevant to the problem:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ijThUi4-wVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "R = 6371\n",
        "pi = np.pi\n",
        "\n",
        "# The distance are normalized such that R = 1\n",
        "\n",
        "def equirectangular(x,y):\n",
        "  dist = 1 * torch.sqrt( (y[1]-x[1]).pow(2) + ( (y[0]-x[0]) * torch.cos( 0.5*(y[1]-x[1]) ) ).pow(2) )\n",
        "  return dist\n",
        "\n",
        "def haversine(x,y):\n",
        "  a = torch.sin( 0.5*(y[1]-x[1]) ).pow(2) + torch.cos(x[1])*torch.cos(y[1])*torch.sin( 0.5*(y[0]-x[0]) ).pow(2)\n",
        "  dist = 2 * torch.atan( torch.sqrt( a/(1-a) ) )\n",
        "  return dist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6lA3cclBoAE",
        "colab_type": "text"
      },
      "source": [
        "$x[0]$ is the tensor of the longitudes ($\\lambda$, radians) \\\\\n",
        "$x[1]$ is the tensor of the latitudes ($\\phi$, radians)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2MeQPuba9YV",
        "colab_type": "text"
      },
      "source": [
        "Load the clusters from the mean-shift algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ1sMZwSal2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "centers = np.load(data_path + \"centers_K.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14crUYGlQlIN",
        "colab_type": "code",
        "outputId": "cb6071ca-d051-4fb8-dd2c-3008dccdfaac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "C = centers.shape[0]\n",
        "print(centers)\n",
        "print(C)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-8.59411598 41.16172906]\n",
            " [-8.60547552 41.12759636]\n",
            " [-8.64602132 41.14424992]\n",
            " ...\n",
            " [-8.65419941 41.18062638]\n",
            " [-8.63515432 41.18811249]\n",
            " [-8.60124582 41.18163732]]\n",
            "2984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7GzEbqOhBj0",
        "colab_type": "text"
      },
      "source": [
        "Partial trajectories that will train our model are chosen to be the first 2k points:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wfo01MQJhCG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhiW2kG1qBG0",
        "colab_type": "text"
      },
      "source": [
        "Define the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QASEJl_CYxjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_dim = 500\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model,self).__init__()\n",
        "    self.fc1 = nn.Linear(in_features=4*k , out_features=hidden_dim).to(device)\n",
        "    self.relu = nn.ReLU().to(device)\n",
        "    self.fc2 = nn.Linear(in_features=hidden_dim, out_features=C).to(device)\n",
        "    self.centers = torch.from_numpy(centers).to(device)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.softmax(x,dim=-1)\n",
        "    cen = self.centers\n",
        "    x = x.mm(cen)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iYqISyspXhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4cmPM0Vr2Dw",
        "colab_type": "text"
      },
      "source": [
        "Loss between two tensors: \\\\\n",
        "$x$ predictions (batch_size,2), $y$ true destinations (batch_size,2) \\\\\n",
        "Convert degrees to radians"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg8jJk22r1CU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(x,y):\n",
        "  return equirectangular(torch.t(x*pi/180),torch.t(y*pi/180)).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxuQEGQ-vp2P",
        "colab_type": "text"
      },
      "source": [
        "Training of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYwiU_p1nNs2",
        "colab_type": "code",
        "outputId": "f12b53fe-f06a-40f4-ad2a-94bb7e547c4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "nb_epochs = 30\n",
        "batch_size = 200\n",
        "lr = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "n_batches = n // batch_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-0c31fc1c7374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mn_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'n' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwbpGVHdUJcP",
        "colab_type": "text"
      },
      "source": [
        "Convert a list of batch_size trajectories into an input array of partial trajectories of shape (batch_size , 2k , 2): \\\\\n",
        "Only consider the first 2k points of each trajectory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDAljWudTyZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_to_partial(X,batch_size=batch_size):\n",
        "  \n",
        "  Y = np.zeros((batch_size,2*k,2))\n",
        "  \n",
        "  for l in range(batch_size):\n",
        "        \n",
        "      if len(X[l]) == 0:                                # if the trajectory does not exist\n",
        "        for j in range(k):                              \n",
        "          Y[l,j,:] = np.array([-8.611317,41.146504])\n",
        "          Y[l,2*k-1-j,:] = np.array([-8.611317,41.146504])\n",
        "\n",
        "      elif len(X[l]) < k:                               # if the trajectory has less than k points\n",
        "        diff = k - len(X[l])\n",
        "        for j in range(diff+1):\n",
        "          Y[l,j,:] = np.array( X[l][0] )\n",
        "          Y[l,2*k-1-j,:] = np.array( X[l][len(X[l])-1] )\n",
        "        for j in np.arange(diff+1,k,1):\n",
        "          Y[l,j,:] = np.array( X[l][j-diff] )\n",
        "          Y[l,2*k-1-j,:] = np.array( X[l][k-1-j] )  \n",
        "        \n",
        "      elif len(X[l]) < 2*k:                              # if the trajectory has more than k points but less than 2k\n",
        "        for j in range(k):\n",
        "          Y[l,j,:] = np.array( X[l][j] )\n",
        "          Y[l,2*k-1-j,:] = np.array( X[l][len(X[l])-1-j] )\n",
        "\n",
        "      else:                                             # if the trajectory has more than 2k points\n",
        "        for j in range(k):\n",
        "          Y[l,j,:] = np.array( X[l][j] )\n",
        "          Y[l,2*k-1-j,:] = np.array( X[l][2*k-1-j] )\n",
        "\n",
        "  return Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w0-208iW6d7",
        "colab_type": "text"
      },
      "source": [
        "Compute the mean and variance (or standard deviation) of the GPS points of all training trajectories with 2k prefixes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaCteo_JXN6E",
        "colab_type": "code",
        "outputId": "584950e4-89aa-4c18-da58-8e8f8ce6af7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mean_v = np.zeros((n_batches,2))\n",
        "var_v = np.zeros((n_batches,2))\n",
        "\n",
        "for i in range(n_batches):\n",
        "  X = train['POLYLINE'][i*batch_size:(i+1)*batch_size]\n",
        "  X = string_to_list(X)\n",
        "  X = list_to_partial(X)\n",
        "  mean_v[i] = np.mean(X,axis=(0,1))\n",
        "  var_v[i] = np.var(X,axis=(0,1))\n",
        "\n",
        "mean = np.mean(mean_v,axis=0)\n",
        "std = np.sqrt(np.mean(var_v,axis=0))\n",
        "print(mean,std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-8.61739435 41.15714826] [0.06085632 0.0222809 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5wObZXXZWSH",
        "colab_type": "text"
      },
      "source": [
        "Training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6lCjj6JwQVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=lr,momentum=momentum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVj21c6jnUZp",
        "colab_type": "code",
        "outputId": "52d59600-b61b-4064-c6d9-2b4cead62680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "model.double()\n",
        "for epoch in range(nb_epochs):\n",
        "  print(epoch)\n",
        "  for i in range(n_batches):\n",
        "    \n",
        "    X = train['POLYLINE'][i*batch_size:(i+1)*batch_size]\n",
        "    X = string_to_list(X)\n",
        "    X = list_to_partial(X)\n",
        "\n",
        "    # Standardized input GPS points\n",
        "    X = ( X - mean ) / std\n",
        "    \n",
        "    X = np.reshape(X,(batch_size,4*k))\n",
        "    X = torch.from_numpy(X).to(device)\n",
        "    \n",
        "    dest_pred = model(X.double())\n",
        "\n",
        "    dest_true = destinations[i*batch_size:(i+1)*batch_size]\n",
        "    dest_true = torch.from_numpy(dest_true).to(device)\n",
        "\n",
        "    loss = loss_fn(dest_pred,dest_true)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFTpcpCiQs2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model,data_path+\"model.dat\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px4ojx3xci71",
        "colab_type": "text"
      },
      "source": [
        "# **Test the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "635TlM-6ZjDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.load(data_path + \"model.dat\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "test = get_data(\"test_new.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuPI4ZY8h84o",
        "colab_type": "text"
      },
      "source": [
        "Get the true destinations of the trajectories in our test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgzOjzi3gC0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_test = len(test['POLYLINE'])\n",
        "\n",
        "destinations_test = np.zeros((n_test,2))\n",
        "for i in range(n_test):\n",
        "  if len(destination(test['POLYLINE'][i])) !=0: # Check if the trajectory (and the destination) actually exists\n",
        "    destinations_test[i,:] = destination(test['POLYLINE'][i])\n",
        "  else:\n",
        "    destinations_test[i,:] = np.array([-8.611317,41.146504]) # If not, we manually choose the destination to be the main avenue\n",
        "\n",
        "partial_test = list_to_partial( string_to_list( test['POLYLINE'] ), batch_size=n_test )\n",
        "\n",
        "# Remove absurd points:\n",
        "\n",
        "if destinations_test.max(axis=0)[0]>-8.4:\n",
        "  arg = np.argmax(destinations_test,axis=0)[0]\n",
        "  destinations_test = np.delete(destinations_test,arg,axis=0)\n",
        "  partial_test = np.delete(partial_test,arg,axis=0)\n",
        "elif destinations_test.max(axis=0)[1]>41.3:\n",
        "  arg = np.argmax(destinations_test,axis=0)[1]\n",
        "  destinations_test = np.delete(destinations_test,arg,axis=0)\n",
        "  partial_test = np.delete(partial_test,arg,axis=0)\n",
        "elif destinations_test.min(axis=0)[0]<-8.8:\n",
        "  arg = np.argmin(destinations_test,axis=0)[0]\n",
        "  destinations_test = np.delete(destinations_test,arg,axis=0)\n",
        "  partial_test = np.delete(partial_test,arg,axis=0)\n",
        "elif destinations_test.min(axis=0)[1]<41.:\n",
        "  arg = np.argmin(destinations_test,axis=0)[1]\n",
        "  destinations_test = np.delete(destinations_test,arg,axis=0)\n",
        "  partial_test = np.delete(partial_test,arg,axis=0)\n",
        "\n",
        "n_test = destinations_test.shape[0]\n",
        "\n",
        "destinations_test = torch.from_numpy(destinations_test).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlrkjnNViDYX",
        "colab_type": "text"
      },
      "source": [
        "Get the corresponding predicted destinations by our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl1cNhipg6CQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "partial_test = (partial_test - mean) / std\n",
        "\n",
        "partial_test = np.reshape(partial_test,(n_test,4*k))\n",
        "partial_test = torch.from_numpy(partial_test).to(device)\n",
        "    \n",
        "destinations_pred = model(partial_test.double())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oII-n5_JiMxd",
        "colab_type": "text"
      },
      "source": [
        "Mean Haversine distance between the predictions and reality:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awNOLjJiiXdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = R * haversine(torch.t(destinations_pred*pi/180),torch.t(destinations_test*pi/180)).mean().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBXNlnx0809m",
        "colab_type": "text"
      },
      "source": [
        "Accuracy in kilometers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slDeoUnD_R9m",
        "colab_type": "code",
        "outputId": "65bfe7d1-7902-4f3a-d7c2-1ebfaf3d22a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(accuracy.item())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4661206369800635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf-XU6Rb85cB",
        "colab_type": "text"
      },
      "source": [
        "Accuracy in degres:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHIgwG4cr5kj",
        "colab_type": "code",
        "outputId": "99b11e65-552e-4b2c-9b22-f6ff83a1eb84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print((destinations_pred-destinations_test).mean(axis=0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.0014, -0.0033], device='cuda:0', dtype=torch.float64,\n",
            "       grad_fn=<MeanBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQh4MhdMckVs",
        "colab_type": "text"
      },
      "source": [
        "# **Another model: with metadata and all possible prefixes for the training set**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGMEESlOc-JY",
        "colab_type": "text"
      },
      "source": [
        "Before explaining the feature engineering, it is important to explain how the program will work. Since we are dealing with 1.7 millions of data in which each data has a complete trajectory (we have to divide all of them in all possible partial trajectory), the RAM memory is not enough to store such amount. Then, we will divide de dataset in several parts (batch) and for each part well determined, we will clean/organize them and train the model.\n",
        "\n",
        "The class **OurFeatureEngineering** will be charged of dividing the dataset, cleaning and organizing each batch, and providing the organized batch (*next_batch*) until there is no more data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVlGjrn7hCY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = get_data(\"train_new.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du5iQXYHJQXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 200\n",
        "k = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDlt1sPPctVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OurFeatureEngineering:\n",
        "\n",
        "  #### ------------------------------\n",
        "  #### Initializing the class\n",
        "  #### ------------------------------\n",
        "  # INPUT\n",
        "  # df_train (pd.DataFrame) -> The full dataset, but we store without the trajectories\n",
        "  # batch_size (int) -> The size of a batch\n",
        "  # k (int) -> The quantity of step at the beginning and the end for the construction of a partial trajectory\n",
        "  # VARIABLES\n",
        "  # list_string (list) -> it stores the string of all complete trajectories\n",
        "  # n_it (int) -> it indicates the next data of the dataset we have to use for the next batch\n",
        "  # length (int) -> it stores the quantity of data in the stored dataset\n",
        "  def __init__(self, df_train, batch_size=batch_size, k=k):\n",
        "    # self.df_train = df_train.loc[:, df_train.columns != 'POLYLINE'].reset_index(drop=True)\n",
        "    # self.list_string = list(df_train[\"POLYLINE\"])\n",
        "    self.df_train, self.list_string = self.df_filter(df_train)\n",
        "    self.batch_size = batch_size\n",
        "    self.k = k\n",
        "    \n",
        "    self.n_it = 0\n",
        "    self.length = self.df_train.shape[0]\n",
        "\n",
        "\n",
        "  #### ------------------------------\n",
        "  #### Filtering the lines and columns of the dataset\n",
        "  #### ------------------------------\n",
        "  def df_filter(self, df):\n",
        "    # Drop the null trajectories\n",
        "    df = df[df[\"POLYLINE\"] != \"[]\"]\n",
        "\n",
        "    # Drop the incomplete trajectories and drop off the MISSING_DATA column\n",
        "    df = df[df[\"MISSING_DATA\"] == False]\n",
        "    df = df.drop(columns=[\"MISSING_DATA\"])\n",
        "\n",
        "    # Change the CALL_TYPE values\n",
        "    df = df.replace({\"CALL_TYPE\": {\"A\":1,\"B\":2, \"C\":3}})\n",
        "\n",
        "    # ORIGIN_CALL -> CLIENT_ID and ORIGIN_STAND -> STAND_ID\n",
        "    df = df.rename(columns={\"ORIGIN_CALL\": \"CLIENT_ID\", \"ORIGIN_STAND\": \"STAND_ID\"})\n",
        "\n",
        "    # Just the CLIENT_ID and STAND_ID can have null values. Since they are\n",
        "    # always greater than 0, we will do null_values -> -1\n",
        "    df = df.fillna(-1)\n",
        "\n",
        "    # Drop off the DAY_TYPE column because all the values are A\n",
        "    df = df.drop(columns=[\"DAY_TYPE\"])\n",
        "\n",
        "    # Changing the values of ORIGIN_CALL, ORIGIN_STAND and TAXI_ID\n",
        "    change_client = {k : v for v,k in enumerate(sorted(df[\"CLIENT_ID\"].unique())) }\n",
        "    change_stand = {k : v for v,k in enumerate(sorted(df[\"STAND_ID\"].unique())) }\n",
        "    change_taxi = {k : v for v,k in enumerate(sorted(df[\"TAXI_ID\"].unique())) }\n",
        "    df[\"CLIENT_ID\"] = df[\"CLIENT_ID\"].map(change_client)\n",
        "    df[\"STAND_ID\"] = df[\"STAND_ID\"].map(change_stand)\n",
        "    df[\"TAXI_ID\"] = df[\"TAXI_ID\"].map(change_taxi)\n",
        "\n",
        "    return df.drop(columns=[\"POLYLINE\"]).reset_index(drop=True), list(df[\"POLYLINE\"])\n",
        "  \n",
        "  #### ------------------------------\n",
        "  #### Providing the next batch of data\n",
        "  #### ------------------------------\n",
        "  # It analyses if it is still possible to provide a batch. If yes, it returns it cleaned/organized\n",
        "  # OUTPUT\n",
        "  # The cleaned/organized batch (pd.DataFrame) \n",
        "  def next_batch(self):\n",
        "    if self.n_it == self.length:\n",
        "      return None\n",
        "\n",
        "    # Any complete batch\n",
        "    if self.n_it + self.batch_size < self.length:\n",
        "      df = self.df_train.iloc[self.n_it : self.n_it+self.batch_size]\n",
        "      self.n_it += self.batch_size\n",
        "    # The last batch (which can have less elements)\n",
        "    else:\n",
        "      df = self.df_train.iloc[self.n_it : self.length]\n",
        "      self.n_it = self.length\n",
        "    \n",
        "    # We organize/clean the batch and return it\n",
        "    return self.preprocessing(df)\n",
        "  \n",
        "\n",
        "  #### ------------------------------\n",
        "  #### Preprocessing the batch dataset\n",
        "  #### ------------------------------\n",
        "  # It calls all the steps to clean/organize the batch dataset\n",
        "  # INPUT\n",
        "  # df (pd.DataFrame) -> a batch dataset\n",
        "  # INTERMEDIATE VARIABLES\n",
        "  # ls (list) -> list of all prefixes of the batch dataset (it is not a list of string \n",
        "  #              like self.list_string, but a list of list of floats)\n",
        "  # OUTPUT\n",
        "  # The batch dataset organized/cleaned (Pd.DataFrame)\n",
        "  def preprocessing(self, df):\n",
        "    df = self.destinations(df)\n",
        "    df, ls = self.all_prefixes(df)\n",
        "    df = self.type_organise(df)\n",
        "    df = self.creation_features(df, ls)\n",
        "    df = self.taking_2k_coordinates(df, ls)\n",
        "    return df.drop(columns=[\"TIMESTAMP\", \"TIMESTAMP_END\", \"TRIP_ID\"])\n",
        "\n",
        "  #### ------------------------------\n",
        "  #### Adding the destination in the batch dataset\n",
        "  #### ------------------------------\n",
        "  # INPUT\n",
        "  # df (pd.DataFrame) -> The batch dataset\n",
        "  # OUTPUT\n",
        "  # the batch dataset with the final destinations (pd.DataFrame)\n",
        "  def destinations(self, df):\n",
        "    dest = []\n",
        "    list_string = [self.list_string[df.index.values.tolist()[i]] for i in range(df.shape[0])]\n",
        "\n",
        "    # For each trajectory in string, we take the destination\n",
        "    for i, string in enumerate(list_string):\n",
        "      list_aux = json.loads(string)\n",
        "      if len(list_aux) > 0:\n",
        "        dest.append(list_aux[-1])\n",
        "      else:\n",
        "        dest.append([0,0])\n",
        "\n",
        "    df_aux = pd.DataFrame(np.array(dest), columns=[\"dest_lon\", \"dest_lat\"], dtype=np.float64)\n",
        "    df_aux = df_aux.set_index(df.index)\n",
        "    return pd.concat([df, df_aux], axis=1, sort=False)\n",
        "\n",
        "\n",
        "  #### ------------------------------\n",
        "  #### Organizing the type of the features of the batch dataset\n",
        "  #### ------------------------------\n",
        "  def type_organise(self, df):     \n",
        "    df[\"TRIP_ID\"] = df[\"TRIP_ID\"].astype('object')\n",
        "    df[\"CALL_TYPE\"] = df[\"CALL_TYPE\"].astype('int64')\n",
        "    df[\"CLIENT_ID\"] = df[\"CLIENT_ID\"].fillna(-1).astype('int64')\n",
        "    df[\"STAND_ID\"] = df[\"STAND_ID\"].fillna(-1).astype('int64')\n",
        "    df[\"TAXI_ID\"] = df[\"TAXI_ID\"].fillna(-1).astype('int64')\n",
        "    df[\"TIMESTAMP\"] = df[\"TIMESTAMP\"].fillna(-1).astype('int64')\n",
        "    df[\"dest_lon\"] = df[\"dest_lon\"].astype('float64')\n",
        "    df[\"dest_lat\"] = df[\"dest_lat\"].astype('float64')\n",
        "    return df\n",
        "\n",
        "\n",
        "  #### ------------------------------\n",
        "  #### Creating all possible prefixes (partial trajectories) of the complete trajectories of the batch dataset\n",
        "  #### ------------------------------\n",
        "  # INPUT\n",
        "  # df (pd.DataFrame) -> The batch dataset\n",
        "  # OUTPUT\n",
        "  # df_new (pd.DataFrame) -> It replicates the input batch dataset according to the number of prefixes.\n",
        "  # list_total (list) -> It is a list that stores the prefixes (it is no more string like self.list_string, \n",
        "  #                      it is a list of floats) with respect to the df_new. \n",
        "  def all_prefixes(self, df):\n",
        "    df_new = pd.DataFrame(columns=df.columns)\n",
        "    length_total = []\n",
        "    list_total = []\n",
        "    list_string = [self.list_string[df.index.values.tolist()[i]] for i in range(df.shape[0])]\n",
        "\n",
        "    # For each trajectory, we create all the prefixes\n",
        "    for i, string in enumerate(list_string):\n",
        "      list_aux = json.loads(string)\n",
        "      length_total.append(len(list_aux))\n",
        "      for j in range(1, len(list_aux) + 1):\n",
        "        list_total.append(list_aux[:j])\n",
        "    \n",
        "    # Replicate the lines in the dataset\n",
        "    df_new = pd.DataFrame(np.repeat(df.values,length_total,axis=0))\n",
        "    df_new.columns = df.columns\n",
        "\n",
        "    return df_new, list_total\n",
        "  \n",
        "\n",
        "  #### ------------------------------\n",
        "  #### Creating other features\n",
        "  #### ------------------------------\n",
        "  # It increases the features of the batch dataset\n",
        "  def creation_features(self, df, ls):\n",
        "    df['LEN'] = [len(ls[i]) for i in df.index.values.tolist()]\n",
        "    df['TIMESTAMP_END'] = df['TIMESTAMP'].map(int) + df['LEN'].map(lambda x: 15*x)\n",
        "    df[\"WEEK\"] = df['TIMESTAMP_END'].map(lambda x: dt.utcfromtimestamp(x).isocalendar()[1]) - 1\n",
        "    df[\"DAY_WEEK\"] = df['TIMESTAMP_END'].map(lambda x: dt.utcfromtimestamp(x).isocalendar()[2]) - 1\n",
        "    df['QUARTER_HOUR'] = df['TIMESTAMP_END'].map(lambda x: 4*dt.utcfromtimestamp(x).hour + dt.utcfromtimestamp(x).minute // 15)\n",
        "    return df\n",
        "  \n",
        "\n",
        "  #### ------------------------------\n",
        "  #### Adding to the batch dataset the k firsts and k lasts coordinates of the prefixes\n",
        "  #### ------------------------------\n",
        "  # INPUT\n",
        "  # df (pd.DataFrame) -> the batch dataset with the replicated lines\n",
        "  # ls (list) -> The list of the prefixes with respect to df.\n",
        "  # OUTPUT\n",
        "  # The initial dataset with the 2k coordinates as features (pd.DataFrame)\n",
        "  def taking_2k_coordinates(self, df, ls):\n",
        "    k = self.k\n",
        "\n",
        "    # Naming the columns\n",
        "    colonnes = []\n",
        "    for i in range(k):\n",
        "      colonnes += [\"lon_\"+str(i+1), \"lat_\"+str(i+1)]\n",
        "    for i in range(k, 0, -1):\n",
        "      colonnes += [\"lon_@\"+str(i), \"lat_@\"+str(i)]\n",
        "\n",
        "    df_aux = pd.DataFrame(columns=colonnes)\n",
        "    \n",
        "    # For each prefixes, we take the 2k points\n",
        "    list_rows = []\n",
        "    for i, list_aux in enumerate(ls):\n",
        "      row = []\n",
        "      n = len(list_aux)\n",
        "      \n",
        "      if n >= k:\n",
        "        for j in range(k):\n",
        "          row += list_aux[j]\n",
        "        for j in range(k):\n",
        "          row += list_aux[-k+j]\n",
        "      elif n>0:\n",
        "        for j in range(k-n):\n",
        "          row += list_aux[0]\n",
        "        for j in range(n):\n",
        "          row += list_aux[j]\n",
        "        for j in range(n):\n",
        "          row += list_aux[-n+j]\n",
        "        for j in range(k-n):\n",
        "          row += list_aux[-1]\n",
        "      else:\n",
        "        continue\n",
        "      list_rows.append(row)\n",
        "\n",
        "    df_aux = pd.DataFrame(np.array(list_rows), columns=colonnes)\n",
        "    df_aux = df_aux.set_index(df.index)\n",
        "    return pd.concat([df, df_aux], axis=1, sort=False)\n",
        "\n",
        "  #### ------------------------------\n",
        "  #### Boolean that says if there are more batch\n",
        "  #### ------------------------------\n",
        "  def has_next(self):\n",
        "    if self.n_it == self.length:\n",
        "      return False\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqDyWFULNQkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reshaping(df, k=k):\n",
        "  coord_col = []\n",
        "  for i in range(k):\n",
        "    coord_col += [\"lon_\"+str(i+1), \"lat_\"+str(i+1)]\n",
        "  for i in range(k, 0, -1):\n",
        "    coord_col += [\"lon_@\"+str(i), \"lat_@\"+str(i)]\n",
        "\n",
        "  dest_col = [\"dest_lon\",\"dest_lat\"]\n",
        "\n",
        "  metadata = [col for col in df.columns if col not in coord_col + dest_col]\n",
        "\n",
        "  return df[metadata], np.array(df[coord_col]).reshape(-1,2*k,2), np.array(df[dest_col])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SBiFoYZGXPF",
        "colab_type": "code",
        "outputId": "bc477fd1-a498-46d5-e408-b2aeb246fa2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "features = OurFeatureEngineering(train)\n",
        "del(train)\n",
        "print(features.df_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         Unnamed: 0              TRIP_ID  ...  TAXI_ID   TIMESTAMP\n",
            "0                 0  1385025300620000107  ...       77  1385025300\n",
            "1                 1  1398009813620000686  ...      426  1398009813\n",
            "2                 2  1400814708620000648  ...      401  1400814708\n",
            "3                 3  1373778035620000600  ...      373  1373778035\n",
            "4                 4  1380261303620000545  ...      343  1380261303\n",
            "...             ...                  ...  ...      ...         ...\n",
            "1664833     1670595  1386552971620000080  ...       60  1386552971\n",
            "1664834     1670596  1384152481620000671  ...      415  1384152481\n",
            "1664835     1670597  1377950684620000142  ...       98  1377950684\n",
            "1664836     1670598  1372898324620000013  ...       12  1372898324\n",
            "1664837     1670599  1388332298620000006  ...        5  1388332298\n",
            "\n",
            "[1664838 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95t5MetGKGUY",
        "colab_type": "text"
      },
      "source": [
        "Distances:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw9euzRgKFek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "R = 6371\n",
        "pi = np.pi\n",
        "\n",
        "# The distance are normalized such that R = 1\n",
        "\n",
        "def equirectangular(x,y):\n",
        "  dist = 1 * torch.sqrt( (y[1]-x[1]).pow(2) + ( (y[0]-x[0]) * torch.cos( 0.5*(y[1]-x[1]) ) ).pow(2) )\n",
        "  return dist\n",
        "\n",
        "def haversine(x,y):\n",
        "  a = torch.sin( 0.5*(y[1]-x[1]) ).pow(2) + torch.cos(x[1])*torch.cos(y[1])*torch.sin( 0.5*(y[0]-x[0]) ).pow(2)\n",
        "  dist = 2 * torch.atan( torch.sqrt( a/(1-a) ) )\n",
        "  return dist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuZBtPnZIhoY",
        "colab_type": "text"
      },
      "source": [
        "Compute and savethe mean and variance (or standard deviation) of the GPS points of all training trajectories with 2k prefixes: \\\\\n",
        "(CAN BE RUN ONLY ONCE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc81Egm6IidA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean_v = []\n",
        "var_v = []\n",
        "\n",
        "while features.has_next():\n",
        "  _,X,_ = reshaping(features.next_batch())\n",
        "  mean_v.append(np.mean(X,axis=(0,1)))\n",
        "  var_v.append(np.var(X,axis=(0,1)))\n",
        "\n",
        "mean_v = np.array(mean_v)\n",
        "var_v = np.array(var_v)\n",
        "mean = np.mean(mean_v,axis=0)\n",
        "std = np.sqrt(np.mean(var_v,axis=0))\n",
        "print(mean,std)\n",
        "features.n_it = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu_a6-slekwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(data_path + 'mean',mean)\n",
        "np.save(data_path + 'std',std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf3OIPj7q8s-",
        "colab_type": "text"
      },
      "source": [
        "Create the embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FxqKXO2rUUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_dim = 10\n",
        "total_client_id = 57106\n",
        "total_taxi_id = 448\n",
        "total_stand_id = 64\n",
        "total_quarter = 96\n",
        "total_day = 7\n",
        "total_week = 52"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0pvSOUtvS4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_client_id = nn.Embedding(total_client_id,embed_dim)\n",
        "embed_taxi_id = nn.Embedding(total_taxi_id,embed_dim)\n",
        "embed_stand_id = nn.Embedding(total_stand_id,embed_dim)\n",
        "embed_quarter = nn.Embedding(total_quarter,embed_dim)\n",
        "embed_day = nn.Embedding(total_day,embed_dim)\n",
        "embed_week = nn.Embedding(total_week,embed_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHdTk68SLHH_",
        "colab_type": "text"
      },
      "source": [
        "Load the clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE9s3dLxLJXj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "centers = np.load(data_path + \"centers_K.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5nUZsCZLSLG",
        "colab_type": "code",
        "outputId": "2491d782-a6b8-48e3-e5c8-934764e384e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "C = centers.shape[0]\n",
        "print(centers)\n",
        "print(C)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-8.59411598 41.16172906]\n",
            " [-8.60547552 41.12759636]\n",
            " [-8.64602132 41.14424992]\n",
            " ...\n",
            " [-8.65419941 41.18062638]\n",
            " [-8.63515432 41.18811249]\n",
            " [-8.60124582 41.18163732]]\n",
            "2984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tHiF5R4CI--",
        "colab_type": "text"
      },
      "source": [
        "New model, with metadata (the input layer has now a shape (batch_size , 4k+60)):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPwRpTpYCISf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_dim = 500\n",
        "\n",
        "class Model_2(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Model_2,self).__init__()\n",
        "    self.fc1 = nn.Linear(in_features=4*k+6*embed_dim , out_features=hidden_dim).to(device)\n",
        "    self.relu = nn.ReLU().to(device)\n",
        "    self.fc2 = nn.Linear(in_features=hidden_dim, out_features=C).to(device)\n",
        "    self.centers = torch.from_numpy(centers).to(device)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "    x = F.softmax(x,dim=-1)\n",
        "    cen = self.centers\n",
        "    x = x.mm(cen)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh_zmjbCFeNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = Model_2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd1L428p1P25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = torch.load(data_path+\"model2_new.dat\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrpfcErfKoRf",
        "colab_type": "text"
      },
      "source": [
        "Loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3wnmH7LKpWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(x,y):\n",
        "  return equirectangular(torch.t(x*pi/180),torch.t(y*pi/180)).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KG5cnXGPKqRk",
        "colab_type": "text"
      },
      "source": [
        "Training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfYdWOSWe846",
        "colab_type": "code",
        "outputId": "5d6df65e-7e46-4daa-b1f1-eaad04169927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mean = np.load(data_path + 'mean.npy')\n",
        "std = np.load(data_path + 'std.npy')\n",
        "print(mean,std)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-8.61687434 41.15820505] [0.04636961 0.05601787]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqqua34hJ0oT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.01\n",
        "momentum = 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaPb0rnPFkmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2.train()\n",
        "optimizer = torch.optim.SGD(model2.parameters(),lr=lr,momentum=momentum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlcIxPlSFxA8",
        "colab_type": "code",
        "outputId": "d1b541b0-6f74-4f6e-cc73-474a6ea0d212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nb_epochs = 1\n",
        "\n",
        "model2.double()\n",
        "for epoch in range(nb_epochs):\n",
        "  features.n_it = 0\n",
        "  print(epoch)\n",
        "  while features.has_next():\n",
        "    \n",
        "    X = features.next_batch()\n",
        "    metadata, X, dest_true = reshaping(X)\n",
        "\n",
        "    # Standardized input GPS points\n",
        "    X = ( X - mean ) / std\n",
        "\n",
        "    X = np.reshape(X,(X.shape[0],4*k))\n",
        "    X = torch.from_numpy(X).to(device)\n",
        "    \n",
        "    meta_client_id = torch.from_numpy(np.array(metadata['CLIENT_ID']))\n",
        "    meta_client_id = embed_client_id(meta_client_id)\n",
        "    meta_client_id = meta_client_id.to(device).double()\n",
        "\n",
        "    meta_taxi_id = torch.from_numpy(np.array(metadata['TAXI_ID']))\n",
        "    meta_taxi_id = embed_taxi_id(meta_taxi_id)\n",
        "    meta_taxi_id = meta_taxi_id.to(device).double()\n",
        "\n",
        "    meta_stand_id = torch.from_numpy(np.array(metadata['STAND_ID']))\n",
        "    meta_stand_id = embed_stand_id(meta_stand_id)\n",
        "    meta_stand_id = meta_stand_id.to(device).double()\n",
        "\n",
        "    meta_quarter = torch.from_numpy(np.array(metadata['QUARTER_HOUR']))\n",
        "    meta_quarter = embed_quarter(meta_quarter)\n",
        "    meta_quarter = meta_quarter.to(device).double()\n",
        "\n",
        "    meta_day = torch.from_numpy(np.array(metadata['DAY_WEEK']))\n",
        "    meta_day = embed_day(meta_day)\n",
        "    meta_day = meta_day.to(device).double()\n",
        "\n",
        "    meta_week = torch.from_numpy(np.array(metadata['WEEK']))\n",
        "    meta_week = embed_week(meta_week)\n",
        "    meta_week = meta_week.to(device).double()\n",
        "\n",
        "    X = torch.cat((X,meta_client_id,meta_taxi_id,meta_stand_id,meta_quarter,meta_day,meta_week),1)\n",
        "\n",
        "    dest_pred = model2(X.double())\n",
        "\n",
        "    dest_true = torch.from_numpy(dest_true).to(device)\n",
        "\n",
        "    loss = loss_fn(dest_pred,dest_true)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  torch.save(model2,data_path+\"model_2.dat\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9L6rYcnhOgB",
        "colab_type": "code",
        "outputId": "d0853f97-21bd-4e31-c329-2b009b9d6de3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#torch.save(model2,data_path+\"model_2.dat\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Model_2. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkP0TVhvgl9t",
        "colab_type": "text"
      },
      "source": [
        "# **Test the model:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-lDVKlUgpMe",
        "colab_type": "code",
        "outputId": "3fb6f4cb-1305-46ed-eae4-8e3ddca69df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "model2 = torch.load(data_path+\"model2_new.dat\")\n",
        "model2.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model_2(\n",
              "  (fc1): Linear(in_features=80, out_features=500, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc2): Linear(in_features=500, out_features=2984, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkHgBaTSPrRp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = get_data(\"test_new.csv\")\n",
        "features_test = OurFeatureEngineering(test.iloc[:500],batch_size=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GilNdtAHSeoN",
        "colab_type": "text"
      },
      "source": [
        "Get the predictions by our model and the true destinations for the testing dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGIOnliBQpcu",
        "colab_type": "code",
        "outputId": "824fc5aa-c2e2-4227-d1e8-66c5970c5a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "features_test.n_it = 0\n",
        "\n",
        "# Count the total number of trajectories in the extended testing dataset (with all possible prefixes)\n",
        "count = 0\n",
        "\n",
        "accuracy = 0.\n",
        "\n",
        "while features_test.has_next():\n",
        "\n",
        "  X = features_test.next_batch()\n",
        "  metadata, X, dest_true = reshaping(X)\n",
        "\n",
        "  # Remove absurd points:\n",
        "\n",
        "  if dest_true.max(axis=0)[0]>-8.4:\n",
        "    arg = np.argmax(dest_true,axis=0)[0]\n",
        "    dest_true = np.delete(dest_true,arg,axis=0)\n",
        "    X = np.delete(X,arg,axis=0)\n",
        "    metadata = metadata.drop(arg)\n",
        "  elif dest_true.max(axis=0)[1]>41.3:\n",
        "    arg = np.argmax(dest_true,axis=0)[1]\n",
        "    dest_true = np.delete(dest_true,arg,axis=0)\n",
        "    X = np.delete(X,arg,axis=0)\n",
        "    metadata = metadata.drop(arg)\n",
        "  elif dest_true.min(axis=0)[0]<-8.8:\n",
        "    arg = np.argmax(dest_true,axis=0)[0]\n",
        "    dest_true = np.delete(dest_true,arg,axis=0)\n",
        "    X = np.delete(X,arg,axis=0)\n",
        "    metadata = metadata.drop(arg)\n",
        "  elif dest_true.max(axis=0)[0]<41.:\n",
        "    arg = np.argmax(dest_true,axis=0)[1]\n",
        "    dest_true = np.delete(dest_true,arg,axis=0)\n",
        "    X = np.delete(X,arg,axis=0)\n",
        "    metadata = metadata.drop(arg)\n",
        "\n",
        "  count += X.shape[0]\n",
        "\n",
        "  # Standardized input GPS points\n",
        "  X = ( X - mean ) / std\n",
        "\n",
        "  X = np.reshape(X,(X.shape[0],4*k))\n",
        "  X = torch.from_numpy(X).to(device)\n",
        "    \n",
        "  meta_client_id = torch.from_numpy(np.array(metadata['CLIENT_ID']))\n",
        "  meta_client_id = embed_client_id(meta_client_id)\n",
        "  meta_client_id = meta_client_id.to(device).double()\n",
        "\n",
        "  meta_taxi_id = torch.from_numpy(np.array(metadata['TAXI_ID']))\n",
        "  meta_taxi_id = embed_taxi_id(meta_taxi_id)\n",
        "  meta_taxi_id = meta_taxi_id.to(device).double()\n",
        "\n",
        "  meta_stand_id = torch.from_numpy(np.array(metadata['STAND_ID']))\n",
        "  meta_stand_id = embed_stand_id(meta_stand_id)\n",
        "  meta_stand_id = meta_stand_id.to(device).double()\n",
        "\n",
        "  meta_quarter = torch.from_numpy(np.array(metadata['QUARTER_HOUR']))\n",
        "  meta_quarter = embed_quarter(meta_quarter)\n",
        "  meta_quarter = meta_quarter.to(device).double()\n",
        "\n",
        "  meta_day = torch.from_numpy(np.array(metadata['DAY_WEEK']))\n",
        "  meta_day = embed_day(meta_day)\n",
        "  meta_day = meta_day.to(device).double()\n",
        "\n",
        "  meta_week = torch.from_numpy(np.array(metadata['WEEK']))\n",
        "  meta_week = embed_week(meta_week)\n",
        "  meta_week = meta_week.to(device).double()\n",
        "\n",
        "  X = torch.cat((X,meta_client_id,meta_taxi_id,meta_stand_id,meta_quarter,meta_day,meta_week),1)\n",
        "\n",
        "  dest_pred = model2(X.double())\n",
        "\n",
        "  dest_true = torch.from_numpy(dest_true).to(device)\n",
        "\n",
        "  accuracy = accuracy + haversine(torch.t(dest_true*pi/180),torch.t(dest_pred*pi/180)).sum().item()\n",
        "  \n",
        "accuracy = accuracy / count\n",
        "print(count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnTeSqpHgjkM",
        "colab_type": "text"
      },
      "source": [
        "Accuracy in kilometers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Igy9x2LjcBIL",
        "colab_type": "code",
        "outputId": "e23f468a-53b6-45ad-d68e-e217bb47211b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(accuracy*R)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.1488310106303405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFuazXaO_rR5",
        "colab_type": "text"
      },
      "source": [
        "Test the first basic model with the same testing set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pkZdrNhtfWv",
        "colab_type": "code",
        "outputId": "6842b49e-3cb9-4196-81fe-070a6b5dd9a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "model = torch.load(data_path+\"model.dat\")\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc1): Linear(in_features=20, out_features=500, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (fc2): Linear(in_features=500, out_features=2984, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQCLEbJO-i2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = get_data(\"test_new.csv\")\n",
        "features_test = OurFeatureEngineering(test.iloc[:1000],batch_size=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlBZrpzd_PDz",
        "colab_type": "code",
        "outputId": "63e37caa-d186-426e-dd0f-3135f069f1af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "features_test.n_it = 0\n",
        "\n",
        "X = features_test.next_batch()\n",
        "metadata, X, dest_true = reshaping(X)\n",
        "X = np.reshape(X,(X.shape[0],4*k))\n",
        "X = torch.from_numpy(X).to(device)\n",
        "dest_pred = model(X.double())\n",
        "dest_true = torch.from_numpy(dest_true).to(device)\n",
        "\n",
        "accuracy = R*haversine(torch.t(dest_true*pi/180),torch.t(dest_pred*pi/180)).mean().item()\n",
        "\n",
        "print(accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.729494895531275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8chFn6iBL4s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}